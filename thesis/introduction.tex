% !TeX root = main.tex
% !TeX spellcheck = en-US
% !TeX encoding = utf8

\chapter{Introduction}
\label{chap:introduction}

\section{Motivation}

The applications of metaheuristics in a world constantly striving for optimization are vast. From finding the shortest path for the vehicle transporting an online purchase \cite{vogel2011flexible}, to routing the traffic from \enquote{Internet of Things} (IoT) devices \cite{sharma2022systematic}, these algorithmic problem solvers are unknowingly omnipresent. As systems become more complex, a demand for metaheuristics has emerged, as they are able to find solutions to underdetermined functions \cite{jamisola2009using}, computationally intensive systems, or NP-hard problems. Even when looking at more mainstream technology topics, especially in data mining or \gls{ml}, metaheuristics play an important role in so-called \textit{hyperparameter optimization} \cite{yang2020hyperparameter}. Metaheuristic algorithms like \gls{pso} and \glspl{ga} are used to find the ideal combination of parameters needed for a \gls{ml} model to perform its best.

According to the \enquote{No free lunch theorem} \cite{wolpert1997no} there cannot exist an optimization algorithm which is perfectly suited for all kinds of problems. Therefore, tackling these pressing research topics requires not one \textit{perfect}, but multiple different metaheuristics. That is especially true, considering the emergence of new problems and challenges, requiring or even imposing a metaheuristic to solve this very optimization problem.

 As a result, this increasingly growing scientific field is not only becoming more convoluted \cite{sorensen2018history}, the algorithmic contexts, not necessarily the algorithms themselves, are also becoming more complex. A streamlined metaheuristic framework, like the \gls{sppbo} proposes \cite{lin2015simple}, can have multiple parameters to choose from and then, ideally, tune to the problem class and instance it shall solve. While problems like the \gls{tsp} or the \gls{qap} have the benefit of being an abstracted version of real-world applicable problems (coming with their own benchmarking packages to boot \cite{reinelt1991tsplib, burkard1997qaplib}), there are plenty of other problems with a multitude of factors to consider, when configuring the parameters for your metaheuristic algorithm. Besides, the real-world is rarely static, therefore, implying the need for solving dynamically changing problems as well.


\section{Problem and Scope}

Knowing the context of modern metaheuristic research, this thesis focuses on the problem arising from feature- and parameter-rich metaheuristic frameworks, exemplified by the aforementioned \gls{sppbo} framework \cite{lin2015simple}. It combines and generalizes aspects from popular swarm intelligence algorithms, namely the \gls{paco} and the \gls{sso}. And while the \gls{sppbo} framework reduces algorithmic complexity, draws similarities to existing metaheuristics and therefore, holds the possibility of solving a greater problem space, it also needs to be configured correctly to perform its best. Solving this task manually, changing the parameters each iteration and looking at the results, is not only inefficient and tedious, but also error-prone, bearing the risk of getting stuck in a local optimum of an multi-dimensional parameter space.

This is also the case for \gls{hsppbo} \cite{kupfer2021hierarchical}, an algorithm derived using the \gls{sppbo} framework and incorporating aspects from \gls{hpso} \cite{janson2003hierarchical}. The hierarchical tree structure organizes a population of \glspl{sce}, which, as the name suggests, each create a solution to the presented problem per iteration - similar to the ants in \gls{paco}. The tree root represents the \gls{sce} with the best solution found so far, branching out into its sibling \glspl{sce} and their less good solutions, and so forth. This structure is subject to change with every new iteration of solutions, establishing a clear hierarchy of influence between the \glspl{sce}. By observing specific swap-patterns of this tree and its \glspl{sce}, the \gls{hsppbo} algorithm is able to detect dynamic changes within the problem instance it solves and reacts accordingly to improve the solution, analyzed similarly in \cite{janson2004hierarchical}.

This opens up the scope of this thesis to dynamically changing problems, like the \gls{dtsp} \cite{psaraftis1995dynamic}. Whereas for the 'normal' symmetrical \gls{tsp} the solution for a given instance of a list or grid of cities would be the shortest path that visits each node (\enquote{city}) exactly once, resulting in a Hamiltonian cycle, in practical applications an exact problem description is often not given in advance. Thus, the \gls{dtsp} is needed to model behavior corresponding with, for example, destinations changing during the routing of vehicles or new cities needing to be visited while the process is already underway.


\section{Approach}
\label{chap:approach}

To summarize, we want to solve the \gls{tsp}, and its dynamic version, using the \gls{hsppbo} algorithm. Furthermore, we want to detect dynamic changes that occur within the problem instances during runtime and react accordingly, to create a newly adapted solution as quickly as possible. And all this with the best available set of parameters. For this last crucial step we take a page from the field of machine learning, where optimizing a model's hyperparameters was already a research topic in the 1990s \cite{feurer2019hyperparameter}. Since then, \gls{hpo} has evolved into an important staple in that research community, being implement into almost every modern \gls{ml} training software and having multiple open-source standalone packages, written in most common programming languages, the most popular being \textit{Python}.
It is precisely this knowledge of optimizing parameters for functions that are often expensive to execute - be it a nondeterministic algorithm or a complex artificial neural network - that we want to leverage for our problem.

In this context, the two arising main research questions are:
\begin{enumerate}
	\item What is the ideal \gls{hpo} method for the \gls{hsppbo} algorithm?
	\item Which sets of parameters yield the best results for a given \gls{dtsp} instance?
\end{enumerate}

This thesis provides a complete software package written in \textit{Python}, containing all the necessary parts needed to answer the research question outlined above. Every aspect of this package is modular (allowing for easy replacement), highly configurable (being able to adapt for other algorithms than \gls{hsppbo}) and well-documented (increasing comprehensibility and replicability of the findings described here).


\section{Outline}

Chapter \ref{chap:related} continues with references to related work and solutions to similar problems, especially concerning dynamic problem solving and parameter tuning for metaheuristics.
Chapter \ref{chap:background} explains the theoretical foundations and knowledge needed to fully understand the methods described.
Complementing this, Chapter \ref{chap:implementation} provides insight into the software implementation, details about the libraries used and makes the algorithms and control flow more understandable in a programmatically oriented way.
Progressing to the research part of the thesis, chapter \ref{chap:experiment} lays out the design of experiments carried out and explains in detail the reasoning behind the selection of problem instances and parameter spaces.
Chapter \ref{chap:results} presents the results and discusses them with regard to the two main research questions.
Lastly, a summary of the work and an outlook on further questions and methods to proceed are given in chapter \ref{chap:conclusion}.






