% !TeX root = main.tex
% !TeX spellcheck = en-US
% !TeX encoding = utf8

\chapter{Introduction}
\label{chap:introduction}

\section{Motivation}

The applications of metaheuristics in a world constantly striving for optimization are vast. From finding the shortest path for the vehicle transporting an online purchase \cite{vogel2011flexible}, to routing the traffic from \enquote{Internet of Things} (IoT) devices \cite{sharma2022systematic}, these algorithmic problem solvers are unknowingly omnipresent. As systems become more complex, a demand for metaheuristics has emerged, as they are able to find solutions to underdetermined functions \cite{jamisola2009using}, computationally intensive systems, or NP-hard problems. Even when looking at more mainstream technology topics, especially in data mining or \gls{ml}, metaheuristics play an important role in so-called \textit{hyperparameter optimization} \cite{yang2020hyperparameter}. Metaheuristic algorithms such as \gls{pso} and \glspl{ga} are used to find the ideal combination of parameters needed for a \gls{ml} model to perform at its best.

According to the \enquote{No free lunch theorem} \cite{wolpert1997no}, there cannot be one optimization algorithm that is perfectly suited for all kinds of problems. Therefore, tackling these pressing research issues requires not one \textit{perfect} metaheuristic, but several different ones. This is especially true when considering the emergence of new problems and challenges that require or even impose a metaheuristic to solve that optimization problem.

 As a result, this increasingly growing scientific field is not only becoming more convoluted \cite{sorensen2018history}, but the algorithmic contexts, not necessarily the algorithms themselves, are also becoming more complex. A streamlined metaheuristic framework, such as the \gls{sppbo} proposes \cite{lin2015simple}, can have multiple parameters to choose from and then, ideally, tune to the problem class and instance it is solving. While problems like the \gls{tsp} or the \gls{qap} have the advantage of being an abstracted version of real-world applicable problems (and come with their own benchmark packages to boot \cite{reinelt1991tsplib, burkard1997qaplib}), there are many of other problems with a variety of factors to consider when configuring the parameters for your metaheuristic algorithm. Moreover, the real-world is rarely static, which means that there is also a need to solve problems that are dynamically changing.


\section{Problem and Scope}

Knowing the context of modern metaheuristics research, this thesis focuses on the problem arising from feature- and parameter-rich metaheuristic frameworks, exemplified by the aforementioned \gls{sppbo} framework \cite{lin2015simple}. It combines and generalizes aspects of popular swarm intelligence algorithms, namely the \gls{paco} and the \gls{sso}. And while the \gls{sppbo} framework reduces algorithmic complexity, draws similarities to existing metaheuristics, and therefore has the potential to solve a larger problem space, it also needs to be configured correctly to perform at its best. Solving this task manually, changing the parameters at each iteration and examining the results, is not only inefficient and tedious, but also error-prone, with the risk of getting stuck in a local optimum of a multidimensional parameter space.

This is also the case for \gls{hsppbo} \cite{kupfer2021hierarchical}, an algorithm derived using the \gls{sppbo} framework and incorporating aspects of \gls{hpso} \cite{janson2003hierarchical}. The hierarchical tree structure organizes a population of \glspl{sce}, which, as the name suggests, each create a solution to the presented problem per iteration, similar to the ants in \gls{paco}. The tree root represents the \gls{sce} with the best solution found so far\footnote{Specifically, the best solution is found at the root of the tree if no new, better solutions have been found in as many iterations as there are levels in the tree.}, branching out to its sibling \glspl{sce} and their less good solutions, and so on. This structure changes with each new iteration of solutions, establishing a clear hierarchy of influence among the \glspl{sce}. By observing specific swap-patterns of this tree and its \glspl{sce}, the \gls{hsppbo} algorithm is able to detect dynamic changes within the problem instance it is solving and react accordingly to improve the solution, as analyzed similarly in \cite{janson2004hierarchical}.

This opens up the scope of this thesis to dynamically changing problems, such as the \gls{dtsp} \cite{psaraftis1995dynamic}. While for the \enquote{normal} symmetric \gls{tsp} the solution for a given instance of a list or grid of cities would be the shortest path that visits each node (\enquote{city}) exactly once, resulting in a Hamiltonian cycle, in practical applications an exact problem description is often not given in advance. Thus, the \gls{dtsp} is needed to model behavior corresponding to, for example, destinations that change during the routing of vehicles, or new cities that need to be visited while the process is already underway.


\section{Approach}
\label{chap:approach}

In summary, we want to solve the \gls{tsp}, and its dynamic version, using the \gls{hsppbo} algorithm. Furthermore, we want to detect dynamic changes that occur within the problem instances during runtime and react accordingly, to create a newly adapted solution as quickly as possible. And all this with the best available set of parameters. For this last crucial step, we take a page from the field of machine learning, where optimizing a model's hyperparameters has been a research topic since the 1990s \cite{feurer2019hyperparameter}. Since then, \gls{hpo} has become an important part of this research community, being implemented in almost every modern \gls{ml} training software and having several open-source standalone packages, written in most common programming languages, with \textit{Python} being one of the most popular choices.
It is precisely this knowledge of parameter optimization for functions that are often expensive to execute - be it a nondeterministic algorithm or a complex artificial neural network - that we want to apply to our problem.

In this context, the two main research questions that arise are:
\begin{enumerate}
	\item What is the ideal \gls{hpo} method for the \gls{hsppbo} algorithm?
	\item Which sets of parameters yield the best results for a given \gls{dtsp} instance?
\end{enumerate}

This thesis provides a complete software package, written in \textit{Python}, that contains all the necessary parts to answer the research question outlined above. Every aspect of this package is modular (allowing for easy replacement), highly configurable (allowing for adaptation to algorithms other than \gls{hsppbo}), and well-documented (increasing the comprehensibility and reproducibility of the results described here).


\section{Outline}

Chapter \ref{chap:related} continues with references to related work and solutions to similar problems, especially concerning dynamic problem solving and parameter tuning for metaheuristics.
Chapter \ref{chap:background} explains the theoretical foundations and knowledge required to fully understand the methods described.
Complementing this, Chapter \ref{chap:implementation} provides insight into the software implementation, details about the libraries used and makes the algorithms and control flow more understandable in a programmatically oriented way.
Moving on to the research part of the thesis, chapter \ref{chap:experiment} describes the design of the experiments performed and explains in detail the reasoning behind the selection of problem instances and parameter spaces.
Chapter \ref{chap:results} presents the results and discusses them with respect to the two main research questions.
Lastly, a summary of the work and an outlook on further questions and methods to proceed are given in chapter \ref{chap:conclusion}.






