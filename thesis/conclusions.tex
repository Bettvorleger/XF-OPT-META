% !TeX root = main.tex
% !TeX spellcheck = en-US
% !TeX encoding = utf8


\chapter{Conclusion}
\label{chap:conclusion}

\glsresetall

In this thesis we successfully applied methods from \gls{hpo} to tune the parameters of the \gls{hsppbo} metaheuristic algorithm to solve the \gls{dtsp}. 
Not only did the parameters obtained by \gls{hpo} perform incredibly well in the dynamic phase of the \gls{dtsp}, but the static performance, just before the problem was changed, also showed admirable performance, especially when compared to a standard set of reference parameters for \gls{hsppbo}.

By planning and implementing all aspects necessary to answer the research questions, a new framework was derived, the \gls{xfopt}. Its main goal is to combine a variety of \gls{hpo} methods, such as \gls{rs}, \gls{bo}, and \gls{gbrt}, with the \gls{hsppbo} metaheuristic, all implemented in the \textit{Python} programming language. Moreover, it provides modularity and interfaces for each of the main modules, the problem, the optimizer, and the metaheuristic, allowing easy extensibility and ensuring potential future development based on the \gls{xfopt} framework. 
In light of the results discussed, it stands as a promising example of the potential that lies in the combination of \gls{hpo} and metaheuristics.

The effort to select appropriate test environments to efficiently perform all the necessary experiments and to analyze the huge amount of data obtained also proved to be worthwhile. The ten \gls{tsp} instances and the three dynamic intensities tested provided a well-rounded subset of problems. While the partitioning of the \gls{tsp} instances from the  \textit{TSPLIB} benchmark set into structural groups was ultimately not observed in the data analyzed, the goal of using disjoint subsets to reduce the number of experiments was still achieved, with all groups successfully generalizing their parameter sets to their larger counterparts. The parameter constraints postulated and used in the optimization process also proved to be correct, with the exception of $\beta$, which showed a clear ceiling effect. And the four \gls{hpo} methods tested presented a diverse selection, performing differently under given circumstances, each with its own advantages. The three parts of the experiments that were conducted all successfully served their purpose of answering the research questions, and during the data analysis no data was felt to be missing, due in large part to the automation of the process using a logging module that captured all the raw data outputs of the processes, and an analysis module that was able to calculate complex tabular results as well as visualize large amounts of data with different aggregations. However, due to the many repetitions of the \gls{hpo} procedure, and despite using two servers in parallel to compute the workload, the experiments took almost three months to complete.

The first research question, the choice of the ideal \gls{hpo} method for the \gls{hsppbo} metaheuristic, was answered in the first part of the results chapter. The arguments provided by convergence plots, \gls{auc} and minimum solution quality metrics, and statistical tests made it clear that \gls{gbrt} was the best choice for \gls{hsppbo}, since it almost always found the best solutions, while converging to them the fastest out of the the four methods compared. Furthermore, \gls{gp} and \gls{et} also showed very promising performance, with \gls{gp} in particular, excelling in \gls{tsp} instances of larger dimension.

The second question of what these best parameter sets look like was answered by the second part of the experiments. Here the results were more ambivalent. Although well-performing parameter sets were found for all specific combinations of problem instance and dynamic intensity, no clear recommendations for parameter values  could be made under most considerations. Except for $\alpha$ and $\beta$, which also seemed to be the most important parameters for the trained \gls{gbrt} surrogate model, and the favored use of the partial reset as the change handling procedure, only improved parameter value ranges were given for the remaining parameters, the three weights, and the detection threshold. In addition, no significant zero order correlation was found between these parameters. 

In the final results chapter, all of these findings were combined - using the best \gls{hpo} method to generate the best parameter values for each combination of problem instance and dynamic intensity - and compared to a well-performing reference parameter set. In terms of solution quality, the \gls{hpo} parameters showed excellent results across all problem descriptions and throughout the runtime. However, it was found that this good solution quality was not due to the correct detection of dynamic changes, but was achieved despite their absence. The reference parameter set outperformed the \gls{hpo} parameters in every discipline regarding dynamic detection, which was explained by either too low or too high choices for the detection threshold $\theta$.

In summary, \gls{hpo} has great potential for tuning the parameters of \gls{hsppbo} and possibly metaheuristics in general. Different optimization methods have different effects on the resulting parameter values and on the problems on which they are obtained on. No single set of parameter values can be recommended to solve all \gls{dtsp} problem descriptions, be they instance or dynamic. However, through the efficient application of \gls{hpo} methods, such a general-purpose parameter set can easily be replaced by a parameter set specifically tuned for a given problem. In the case of \gls{hsppbo}, it is debatable whether good performance on the \gls{dtsp} should be achieved by constructing highly greedy-influenced \glspl{sce}, or by correct detection of dynamic events, and also which parameters influence the solution quality the most.

\section{Future Work}

On many occasions throughout the thesis, thoughts were given about possible future work and research. 
To further improve the solution quality, new parameter value ranges and fixed value recommendations were presented (see \cref{chap:param-recommend}), which are an ideal starting point for new experiments with any kind of \gls{hpo} method. In this context, the good performance of the \gls{gp} method for \gls{tsp} instances of larger dimension ($>150$) could be further analyzed, especially in comparison with \gls{gbrt}. Also, a cross-validation of \gls{hpo} parameter sets obtained for different problem descriptions could be done to investigate, how specific they are tied to their original problem and how well they can be generalized. 
Since the optimization results focus on the parameters that lead to the best solution quality, the experiments were also prepared with this goal in mind. This raises two interesting topics that could not be answered with the available data. First, the tuning of the \gls{hpo} process itself could significantly reduce the time needed for a parameter set. Thus, experiments should be conducted on the minimum number of objective calls required, different initial sampling methods, and other ways to make \gls{hpo} more suitable for online use. The second possibility for future work to optimize the \gls{hpo} process is the evaluation of different solution scoring functions $f(\mathbf{\lambda})$, which could strongly influence what kind of \enquote{good} solutions are found. 
This is also where the lack of dynamic performance can be addressed by including the correct detection of dynamic events in the evaluation function that gives feedback to \gls{hpo}. Implementing the precision and recall metrics described above in the function would be one way to do this.

Finally, the data sets obtained in this thesis offer an intriguing possibility of replacing \gls{hpo} by using \gls{ml} models for parameter inference in a metaheuristic algorithm. The experiments resulted in numerous parameter sets that produced near-optimal solutions, along with detailed meta-information about each run, problem instance, city placement properties, and dynamic aspects. While the problem description was found to have a significant influence on the optimal parameters, direct correlations between parameter values and problems were not observed, suggesting higher-order relationships. Therefore, a \gls{ml} model capable of capturing these complex relationships is proposed. The training set would include experimental data, particularly from the second part, with input features represented by meta-information about the problem environment ($R$, $\lambda_1$, or $CQV$). The dynamics could be described by specific measures ($C$ or $T_d$) or generalized categories. The output or label would be the chosen parameters for a given problem description. Two possible approaches for training the model can be used: supervised learning using the best parameter sets only, or reinforcement learning that also incorporates poorly performing sets. Supervised learning, while limited by available \gls{hpo} data, offers easier preprocessing and implementation, potentially achieving satisfactory accuracy with simple methods. On the other hand, reinforcement learning requires more data and training time but may lead to better accuracy and generalization. Regardless of the chosen ML method, the trained model would enable inferring optimal parameter values for any \gls{tsp} problem description.

However good this \glsdesc{ml} option might be, training such a model would be a challenge in and of itself, not to mention the training data that would be required. In contrast, the possibility of \gls{hpo} for metaheuristics, discussed here and supported by evidence, should be the primary focus of future work.