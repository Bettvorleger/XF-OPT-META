% !TeX root = main.tex
% !TeX spellcheck = en-US
% !TeX encoding = utf8

\chapter{Related Work}
\label{chap:related}

\section{Metaheuristics on Dynamic Problems}

One of the first publications to propose the \gls{dtsp} as a potential and relevant problem was the work of \citet{psaraftis1988dynamic} in 1988, which mentioned \enquote{interchange methods}, such as the 2-opt \cite{croes1958method}, 3-opt \cite{lin1965computer}, or Lin-Kernighan \cite{lin1973effective} methods, for solving slow dynamic changes occurring in the \gls{tsp}.
The use of metaheuristics for dynamic problems began its early development starting in the 2000s. The theoretical concept of the \gls{dtsp} was discussed by \citet{huang2001dynamic}. Subsequently, the work of \citet{angus2002ant} showed, that adapting the \gls{aco} to a dynamic change of the \gls{tsp} is faster than restarting the algorithm, while \citet{guntsch2001pheromone} already proposed a general method for the \gls{aco} using modified pheromone diversification strategies to counteract the random insertion or deletion of cities in a \gls{tsp} instance. Similar methods have been proposed by \citet{eyckelhof2002ant}.
In contrast to changing the node topology of the \gls{tsp}, \citet{silva2004ant} applied dynamic constraints to the nodes and left the evaluation to the ants.
More recent work on the use of \gls{aco} on different versions of the \gls{dtsp} has been done by \citet{mavrovouniotis2013ant}, \cite{mavrovouniotis2016ant}.
 
Examining other metaheuristic categories, \citet{li2006new} solve a version of the \gls{dtsp} by using an evolutionary algorithm that applies genetic-like operations of inversion and recombination. Another proposal in the field of \gls{ga} is the work of \citet{simoes2011chc}, who use an algorithm based on CHC  (\enquote{Cross-generational elitist selection, Heterogeneous recombination, and Cataclysmic mutation} \cite{eshelman1991chc}) to solve a dynamic version of the \gls{tsp} with changes in edge weights and insertions, deletions, and swapping of city nodes.

\gls{pso} has also been successfully used to react to dynamic changes in works by \citet{janson2004hierarchical},  \cite{janson2006hierarchical}. Several variants of the \gls{pso} - \gls{hpso} and a partitioned version (PH-PSO) - were used to not only solve, but also to detect changes within the presented dynamic problem instances.

\section{Parameter Optimization for Metaheuristics}
\label{chap:paramopt}

The choice of parameters has always been an important consideration in metaheuristics research. \citet{eiben1999parameter} give a thorough review and analysis of the different options in the field of parameter settings. Although they focus on \glspl{ga}, they provide a useful taxonomy for parameter optimization. They propose a distinction between \textit{parameter tuning}, where the parameters are fixed before runtime, and \textit{parameter control} strategies, where the values vary during algorithm runtime. \textit{Parameter control} methods are then distinguished by their type of value modification. \citeauthor{eiben1999parameter} conclude that a \textit{parameter control} strategy usually leads to better solutions compared to \textit{parameter tuning}. \citet{talbi2009metaheuristics} made a similar classification, including a further distinction between (offline) parameter tuning methods.
Lastly, in their review of parameter optimization, \citet{wong2008parameter} conclude that parameter tuning plays an important role in the exploratory and exploitative behavior of \gls{aco}.

The most popular reference in manual parameter tuning for \gls{aco} is the original work by \citeauthor{dorigo1991ant}  \cite{dorigo1991ant,dorigo1996ant}. The resulting values from several experiments with different parameter combinations performed on a \gls{tsp} instance serve as a baseline for many subsequent studies. An updated version of these \enquote{good parameters}, including variations of the original \gls{aco}, was published by \citet{dorigo2004ant}.
These parameter combinations were then challenged by \citet{gaertner2005optimal}, who classified the \gls{tsp} instances by certain properties, and then tried a wide range of parameters for these new \gls{tsp} categories. They found optimal parameter values, in some cases very different from those originally proposed.
More recent work in the area of parameter tuning is done by \citet{tuani2018h}, where, for a heterogeneous \gls{aco}, each ant is initialized with different random distributions for the $\alpha$ and $\beta$ values. The algorithm has been tested on several \gls{tsp} instances and compared against different \gls{aco} variants and their heterogeneous counterparts.

 A thorough review, classification, and research on online parameter control has been done by the aforementioned \citet{stutzle2012parameter}. Based on the taxonomy proposed by \citeauthor{eiben1999parameter}, they modify and apply these categories to other approaches in the field of \gls{aco} parameter control. In addition, their experiments with deterministic parameter control strategies take into account the computational time and the anytime behavior of the algorithms. Further research on this type of parameter optimization has been done by \citet{neyoy2013dynamic}, where fuzzy logic statements are used to improve the solution diversification behavior. 
 A self-adaptive control scheme has been proposed by \citet{hao2006adaptive}, which constructs a combinatorial problem of the parameter search and applies \gls{pso} to optimize them in each iteration. Benchmarks using the \gls{tsp} promise good results. Similar studies have been done by \citet{li2016parameter}, with a version of the \gls{aco} whose parameters are controlled by a bacterial foraging algorithm, and compared to parameter control by \gls{ga} and \gls{pso}.
 Other interesting developments include the work of \citet{randall2004near}, who constructed an almost parameter-free version of the \gls{aco}.
 
One of the first analogies drawn between optimizing metaheuristics and \gls{ml} can be found in \citetitle{birattari2009tuning} by \citet{birattari2009tuning}. They analyzed the similarities to problems faced by supervised learning and proposed guidelines for sampling parameter sets. Finally, they applied their results using a version of the Hoeffding race algorithm \cite{maron1993hoeffding} (originally used to select good models, of which \gls{hpo} is a subset) to, among other things, tune the parameters of a MAXâ€“MIN Ant System to solve the \gls{tsp}.
Another approach in the context of applying \gls{ml} concepts to parameter optimization was proposed by \citet{dobslaw2010parameter}, who explained the possibility of training an \gls{ann} on the relationship between the characteristics of a problem instance and a parameter set that led to good solutions. The necessary training data is to be obtained using the \gls{doe} framework. In this sense, several other authors have also proposed some form of \gls{doe} variant, in this case the Taguchi method, to optimize or select \gls{ml} models and/or their hyperparameters, see \cite{packianather2000optimizing, tortum2007investigation, jung2011artificial}.

Finally, an approach similar to this thesis, but more narrowly focused, was investigated by \citet{yin2021bayesian}. They used two hyperparameter optimization methods, \gls{rs} and \gls{bo}, to tune the parameters of a classical \gls{aco} algorithm on multiple instances of the asymmetric \gls{tsp}. Their results promise great potential for tuning parameters in this way, without requiring a priori knowledge of the problem or the algorithm.