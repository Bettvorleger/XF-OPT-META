% !TeX root = main.tex
% !TeX spellcheck = en-US
% !TeX encoding = utf8

\chapter{Related Work}
\label{chap:related}

\section{Metaheuristics on Dynamic Problems}

One of the first publications to propose the \gls{dtsp} as a potential and relevant problem was the work of \citet{psaraftis1988dynamic} in 1988, mentioning \enquote{interchange methods}, like the 2-opt \cite{croes1958method}, 3-opt \cite{lin1965computer} or Lin-Kernighan \cite{lin1973effective} methods, for solving slow dynamic changes occurring in the \gls{tsp}.
Using metaheuristics for dynamic problems began its early development starting in the 2000s. The theoretical concept of the \gls{dtsp} was discussed by \citet{huang2001dynamic}. Following this, the work of \citet{angus2002ant} indicated, that adapting the \gls{aco} to a dynamic change of the \gls{tsp} is faster than restarting the algorithm, while \citet{guntsch2001pheromone} already proposed a general method for the \gls{aco} using modified pheromone diversification strategies to counteract the random insertion or deletion of cities in a \gls{tsp} instance. Similar methods were suggested by \citet{eyckelhof2002ant}.
Unlike changing the node topology of the \gls{tsp}, \citet{silva2004ant} applied dynamic constrains to the nodes and left the evaluation to the ants.
More recent reviews regarding the usage of \gls{aco} on different versions of the \gls{dtsp} were done by \citet{mavrovouniotis2013ant}, \cite{mavrovouniotis2016ant}.

Examining other metaheuristic categories, \citet{li2006new} solve a version of the \gls{dtsp} by using an evolutionary algorithm that applies genetic-like operations of inversion and recombination. Another proposal in the field of \gls{ga} is the work of \citet{simoes2011chc}, using an algorithm based on CHC  (\enquote{Cross-generational elitist selection, Heterogeneous recombination, and Cataclysmic mutation} \cite{eshelman1991chc}) to solve a dynamic version of the \gls{tsp} with changes to edge weights and insertions, deletions and swapping of city nodes.

\gls{pso} was also successfully used to react to dynamic changes in works by \citet{janson2004hierarchical},  \cite{janson2006hierarchical}. Several variants of the \gls{pso} - \gls{hpso} and a partitioned version (PH-PSO) - were used to not only solve, but additionally detect changes within the dynamic problem instances presented.

\section{Parameter Optimization for Metaheuristics}
\label{chap:paramopt}

The choice of parameters was always an important consideration in research of metaheuristics. \citet{eiben1999parameter} give an in-depth review and analysis over the different options in the field of parameter settings. Although focusing on \glspl{ga}, they create a useful taxonomy for parameter optimization. They propose a distinction between \textit{parameter tuning}, where the parameters are fixed before runtime, and \textit{parameter control} strategies, where the values vary during algorithm runtime. \textit{parameter control} are then distinguished by their type of value modification. \citeauthor{eiben1999parameter} come to the conclusion, that a \textit{parameter control} strategy usually results in better solutions compared to \textit{parameter tuning}. \citet{talbi2009metaheuristics} did a similar classification, including further distinction between (offline) parameter tuning methods.
Lastly, In their review of parameter optimization, \citet{wong2008parameter} conclude that parameter tuning plays an important role in the exploratory and exploitative behavior of \gls{aco}.

The most popular reference in manual parameter tuning for \gls{aco} is the original work by \citeauthor{dorigo1991ant}  \cite{dorigo1991ant,dorigo1996ant}. The resulting values of several experiments with varying parameter combinations performed on one \gls{tsp} instance serve as a baseline for many following studies. An updated version of 'good parameters' including variations of the original \gls{aco} was published by \citet{dorigo2004ant}.
These parameter combinations were then challenged by \citet{gaertner2005optimal}, who classified the \gls{tsp} instances by certain properties, and then tried out wide ranges of parameters for these new \gls{tsp} categories. They found optimal parameter values that, in some cases, differ greatly from the originally proposed values.
More recent work in the field of parameter tuning is done by \citet{tuani2018h}, where, for a heterogeneous \gls{aco}, each ant is initialized with different random distributions for the $\alpha$ and $\beta$ values. The algorithm was tested on multiple \gls{tsp} instances and compared against different \gls{aco} variant and their heterogeneous counterparts.

 A thorough review, classification and research on online parameter control was done by the aforementioned \cite{stutzle2012parameter}. Building up on the taxonomy proposed by \citeauthor{eiben1999parameter}, they modify and apply these categories to other approaches in the field of \gls{aco} parameter control. Furthermore, computational time and any-time behavior of algorithms are also factored in for their experiments done with deterministic parameter control strategies. Further research on this type of parameter optimization was done by \citet{neyoy2013dynamic}, where fuzzy logic statements are used to improve the solution diversification behavior. 
 A self-adaptive control scheme was proposed by \citet{hao2006adaptive}, constructing a combinatorial problem of the parameter search and applying \gls{pso} to optimize them in each iteration. Benchmarks using the \gls{tsp} promise good results. Similar studies were done by \citet{li2016parameter}, with a version of the \gls{aco} having its parameters controlled by a bacterial foraging algorithm and compared against parameter control through \gls{ga} and \gls{pso}.
 Other interesting developments include the work of \citet{randall2004near}, who construct an almost parameter free version of the \gls{aco}.
 
One of the first analogies drawn between optimizing metaheuristics and \gls{ml} can be found in \citetitle{birattari2009tuning} by \citet{birattari2009tuning}. They analyze the similarities to problems that supervised learning also faces and propose guidelines for sampling parameter sets. Finally, they apply their findings using a version of the Hoeffding race algorithm \cite{maron1993hoeffding} (originally used to select good models, of which \gls{hpo} is a subset) to, among other examples, tune the parameters of a MAXâ€“MIN Ant System to solving the \gls{tsp}.
A different approach in the context of applying \gls{ml} concepts to parameter optimization was proposed by \citet{dobslaw2010parameter}, explaining the possibility to train an \gls{ann} on the relation between the characteristics of a problem instance and a parameter set that resulted in good solutions. The necessary training data is to be acquired using the \gls{doe} framework. On that note, several other authors also proposed some form of \gls{doe} variant, in this case the Taguchi method, to optimize or select \gls{ml} models and/or their hyperparameters, see \cite{packianather2000optimizing, tortum2007investigation, jung2011artificial}.

Lastly, a similar approach to this thesis, albeit more narrow in scope, was researched by \citet{yin2021bayesian}. They used two hyperparameter optimization methods, random search and \gls{bo}, for tuning the parameters of a classic \gls{aco} algorithm on several instances of the asymmetric \gls{tsp}. Their results promise great potential for tuning parameters this way, without requiring a priori knowledge of the problem.